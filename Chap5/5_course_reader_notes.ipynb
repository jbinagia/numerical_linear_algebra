{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "* LU better for actually solving linear systems \n",
    "* QR better for revealing information about the system of equation \n",
    "* if a matrix $Q$ is orthogonal, then it can only rotate/reflect a vector\n",
    "\n",
    "### Applications of QR factorization\n",
    "1. Solving least-squares \n",
    "2. Used in computing eigenvalues and singular values \n",
    "3. Used in iterative methods for solving systems of equations \n",
    "\n",
    "## Householder reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](qr-choices.png \"Q can be square or rectangular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Method of choice for $Q$ square (i.e. I and III above)\n",
    "* What operations must we perform on $A$ to recover $R$? The product of these elementary operations must be $Q^T$ i.e. $Q^TA=R$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Householder reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We begin with the first column of $R$. I.e. what matrix must we pre-multiply the first column of A by to retrive some multiple of $e_1$, the unit vector in the 1-direction\n",
    "* Idea is to find a reflection that maps $a_1$ to $\\pm\\Vert a_1\\Vert e_1$\n",
    "* reflection is defined by a plane, which itself is defined by a normal vector\n",
    "\n",
    "What is the formula for this reflection?\n",
    "1. Project $x$ onto normal vector $v$: $y = \\frac{v^T x}{v^T v} v$\n",
    "2. Target is $x - 2 y = x - \\frac{v^T x}{v^T v} v = (I - 2\\frac{v v^T}{v^T v})x$\n",
    "Hence the projection operator $P$ is $I - 2\\frac{v v^T}{v^T v}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how do we choose $v$ such that we reflect $x$ onto $\\Vert x \\Vert e_1$?\n",
    "Want $ Px = \\Vert x \\Vert e_1 = x - \\beta v v^T x$ where $\\beta = 2/(v^T v)$. Rearranging we have $\\beta (v^T x) v = x - \\Vert x \\Vert e_1$. Direction is all that matters so choose $\\beta (v^T x) = 1$ implying $v = x - \\Vert x \\Vert e_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roundoff errors\n",
    "* Should reflect onto -$e_1$ rather than $e_1$ to avoid large cancellation errors (i.e. large roundoff errors) seeing as nominally $v_1 = x_1 - \\Vert x \\Vert$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Givens rotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Useful when you only need to make small changes to a matrix (editing just a few entries at a time) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gram-Schmidt\n",
    "* Used when we want a tall skinny $Q$ rather than a square one \n",
    "* Consider each column of $A$ and note that it lies in the span of the first $k$ columns of $Q$ (coefficients given by $k$-th column of R)\n",
    "* For column 1 this means $a_1 = r_{11} q_1$. Now $q_1$ must be unit size so we take $q_1 = a_1/\\Vert a_1 \\Vert$ and $r_{11} = \\Vert a_1 \\Vert$ \n",
    "* Thus in general we have $a_k = r_{kk}q_k + \\sum_{i=1}^{k-1} r_{ik}q_i$ and we don't know $r_{ik}$ (the $k$-th column vector of $R$) and $q_k$ at the $k$-th step.\n",
    "* Because the columns of $Q$ are orthonormal though we know that for $i<k$ $r_{ik}$ is given by: $r_{ik} = q_{i}^T a_k$\n",
    "* To find $r_{kk}$ we rearrange the above as: $z=r_{kk}q_k = a_k - \\sum_{i=1}^{k-1} r_{ik}q_i$. $q_k$ is a unit vector so this means $r_{kk}=\\Vert z \\Vert$ and furthermore $q_k = z/r_{kk}$\n",
    "* Key to making this algorithm stable is updating the entries of the $k$-th column of $A$ as the entries in $R$ are computed (rather than all at once at the end of the $k$-th step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical Gram-Schmidt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0 2.0 3.0; 4.0 5.0 6.0; 7.0 8.0 9.0]"
     ]
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "include(\"../src/geqrf.jl\")\n",
    "\n",
    "A = [1.0 2 3; 4 5 6; 7 8 9]; # simple example \n",
    "R = geqrfGS!(A)\n",
    "print(A*R) # get A back "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pretty_printing (generic function with 1 method)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Printf\n",
    "\n",
    "function pretty_printing(A)\n",
    "    for i = 1:size(A,1)\n",
    "        for j = 1:size(A,2)\n",
    "            @printf(\"%10.2e \",A[i,j]) # 2 characters to be printed \n",
    "                # having 2 decimal places in scientific (e) notation\n",
    "        end\n",
    "        @printf(\"\\n\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately classical Gram-Schmidt is unstable, as shown by the following example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.00e+00   1.00e+00   1.00e+00 \n",
      "  1.49e-08   0.00e+00   0.00e+00 \n",
      "  0.00e+00   1.49e-08   0.00e+00 \n",
      "  0.00e+00   0.00e+00   1.49e-08 \n",
      "dot(Q[:, 2], Q[:, 3]) = 0.4999999999999999\n",
      "norm(Q[:, 1]) = 1.0\n"
     ]
    }
   ],
   "source": [
    "ϵ = sqrt(eps(Float64)); # sqrt of unit roundoff error \n",
    "A = [1 1 1; ϵ 0 0; 0 ϵ 0; 0 0 ϵ];\n",
    "pretty_printing(A)\n",
    "\n",
    "R = geqrfGS!(A)\n",
    "Q = A\n",
    "@show dot(Q[:,2],Q[:,3]) # columns of Q are not orthogonal!\n",
    "@show norm(Q[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because the first column mathematically has a norm greater than 1 but the computer views it as having a norm of 1 due to roundoff errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.00e+00   1.00e+00   1.00e+00 \n",
      "  2.98e-08   0.00e+00   0.00e+00 \n",
      "  0.00e+00   2.98e-08   0.00e+00 \n",
      "  0.00e+00   0.00e+00   2.98e-08 \n",
      "dot(Q[:, 2], Q[:, 3]) = -4.996003610813204e-16\n",
      "norm(Q[:, 1]) = 1.0\n"
     ]
    }
   ],
   "source": [
    "ϵ = 2*sqrt(eps(Float64)); # slightly larger than before \n",
    "A = [1 1 1; ϵ 0 0; 0 ϵ 0; 0 0 ϵ];\n",
    "pretty_printing(A)\n",
    "\n",
    "R = geqrfGS!(A)\n",
    "Q = A\n",
    "@show dot(Q[:,2],Q[:,3]) # columns of Q are orthogonal now!\n",
    "@show norm(Q[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve the stability of the algorithm by project the $j$-th column of A to be orthogonal to the prior columns of $Q$ *while* we are finding the entries of R in column $j$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.00e+00   1.00e+00   1.00e+00 \n",
      "  1.49e-08   0.00e+00   0.00e+00 \n",
      "  0.00e+00   1.49e-08   0.00e+00 \n",
      "  0.00e+00   0.00e+00   1.49e-08 \n",
      "dot(Q[:, 2], Q[:, 3]) = 1.1102230246251565e-16\n",
      "norm(Q[:, 1]) = 1.0\n"
     ]
    }
   ],
   "source": [
    "ϵ = sqrt(eps(Float64)); # sqrt of unit roundoff error \n",
    "A = [1 1 1; ϵ 0 0; 0 ϵ 0; 0 0 ϵ];\n",
    "pretty_printing(A)\n",
    "\n",
    "R = geqrfMGS!(A)\n",
    "Q = A\n",
    "@show dot(Q[:,2],Q[:,3]) # columns of Q are orthogonal now!\n",
    "@show norm(Q[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least-square problems \n",
    "* The case where there are more equations than unknown (so tall and skinny A)\n",
    "* What $x$ makes $A x$ closest to $b$ \n",
    "* One way is *method of normal equations*: solve $A^T A x = A^T b$. \n",
    "* Not good if $A$ is ill-conditioned though since then condition number of $A^T A$ is twice that value\n",
    "\n",
    "### QR method for least-square problems\n",
    "* Idea is analagous to that for the method of normal equations except we note that $R(A) = R(Q)$ and thus start from $Q^T A x = Q^T b$ since we know $Q$ is well-conditioned.\n",
    "* $A = QR$ so really this becomes $R x = Q^T b$\n",
    "\n",
    "### Rank-deficient A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this case there are infinite number of solutions that satisfy the normal-equations.\n",
    "* Instead what we do is use SVD to find the unique solution that has minimal 2-norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
